{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhS55880V07"
      },
      "source": [
        "# Token Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFeaKdsX0tbV",
        "outputId": "6d947485-eab3-4e0e-8e8c-c2874d260bc2"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets seqeval wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctqE_noZ0cZK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer,\n",
        "                          DataCollatorForTokenClassification)\n",
        "from transformers import pipeline\n",
        "from seqeval.metrics import classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQaPln2n0hUU"
      },
      "source": [
        "### Example of Token Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oX_e4xAm0kGo",
        "outputId": "2be749b2-bd8d-411c-dbcc-77d265de7187"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Token Classification Output:\n",
            "{'entity': 'I-PER', 'score': 0.9996063, 'index': 1, 'word': 'El', 'start': 0, 'end': 2}\n",
            "{'entity': 'I-PER', 'score': 0.9991404, 'index': 2, 'word': '##on', 'start': 2, 'end': 4}\n",
            "{'entity': 'I-PER', 'score': 0.9993499, 'index': 3, 'word': 'Mu', 'start': 5, 'end': 7}\n",
            "{'entity': 'I-PER', 'score': 0.99846965, 'index': 4, 'word': '##sk', 'start': 7, 'end': 9}\n",
            "{'entity': 'I-ORG', 'score': 0.9976349, 'index': 9, 'word': 'Te', 'start': 24, 'end': 26}\n",
            "{'entity': 'I-ORG', 'score': 0.9928018, 'index': 10, 'word': '##sla', 'start': 26, 'end': 29}\n",
            "{'entity': 'I-LOC', 'score': 0.9963245, 'index': 16, 'word': 'Pa', 'start': 57, 'end': 59}\n",
            "{'entity': 'I-LOC', 'score': 0.9900282, 'index': 17, 'word': '##lo', 'start': 59, 'end': 61}\n",
            "{'entity': 'I-LOC', 'score': 0.9958049, 'index': 18, 'word': 'Alto', 'start': 62, 'end': 66}\n"
          ]
        }
      ],
      "source": [
        "example_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "example_text = \"Elon Musk is the CEO of Tesla, which is headquartered in Palo Alto.\"\n",
        "\n",
        "output = example_pipeline(example_text)\n",
        "\n",
        "print(\"\\nToken Classification Output:\")\n",
        "for entity in output:\n",
        "    print(entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsYFsZUGcQu2"
      },
      "source": [
        "We can use aggregation_strategy=\"simple\" to merge subword tokens into whole words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVJFV6d2aWXX",
        "outputId": "fd5cd154-f0cd-4e24-cbdb-6432758608b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Token Classification Output:\n",
            "{'entity_group': 'PER', 'score': 0.9991415, 'word': 'Elon Musk', 'start': 0, 'end': 9}\n",
            "{'entity_group': 'ORG', 'score': 0.99521834, 'word': 'Tesla', 'start': 24, 'end': 29}\n",
            "{'entity_group': 'LOC', 'score': 0.9940526, 'word': 'Palo Alto', 'start': 57, 'end': 66}\n"
          ]
        }
      ],
      "source": [
        "example_pipeline = pipeline(\"token-classification\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
        "\n",
        "output = example_pipeline(example_text)\n",
        "\n",
        "print(\"\\nToken Classification Output:\")\n",
        "for entity in output:\n",
        "    print(entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TndTfytdjR1"
      },
      "source": [
        "### Full Token Classification Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9WCtE2Ld_nt"
      },
      "source": [
        "1- Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQSuPOGgeE97",
        "outputId": "172723f5-5a24-41e6-e2c0-cf249c19d8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label list: ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None)\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"conll2003\")\n",
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "print(\"Label list:\", dataset[\"train\"].features[\"ner_tags\"].feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzj5gXbOeRI3"
      },
      "source": [
        "2- Tokenizer Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "SR-Q-uqfeSRd"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uk2ctPRHeZ9S"
      },
      "source": [
        "3- Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "SulVLhz3ecYx"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word = None\n",
        "        label_ids = []\n",
        "        for word_id in word_ids:\n",
        "            if word_id is None:\n",
        "                label_ids.append(-100)  # Special tokens get -100 label\n",
        "            elif word_id != previous_word:\n",
        "                label_ids.append(label[word_id])  # First token of the word gets the label\n",
        "            else:\n",
        "                label_ids.append(label[word_id])  # Subsequent subwords get the same label\n",
        "            previous_word = word_id\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmaPVJr6eszm"
      },
      "source": [
        "4- Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCwyH5pneult",
        "outputId": "36ae14e4-1537-414b-c77e-2111fb9c397b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "label2id = {label: i for i, label in id2label.items()}\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(label_list), id2label=id2label, label2id=label2id,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UIhEEUoe06x"
      },
      "source": [
        "5- Data Collation & Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "x_aOdeLEe3Yk"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KbiuaY4e7Wy"
      },
      "source": [
        "6- Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "nKbQkASQe9Vk"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in labels]\n",
        "    pred_labels = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "                   for prediction, label in zip(predictions, labels)]\n",
        "    report = classification_report(true_labels, pred_labels, output_dict=True)\n",
        "    return {\n",
        "        \"f1\": report[\"micro avg\"][\"f1-score\"],\n",
        "        \"precision\": report[\"micro avg\"][\"precision\"],\n",
        "        \"recall\": report[\"micro avg\"][\"recall\"],\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JgCmVsSfA5t"
      },
      "source": [
        "7- Model Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "eJq6klJ4fDKu",
        "outputId": "660299f9-a98b-416d-e70b-fc1048ff759b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5268' max='5268' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5268/5268 10:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.080400</td>\n",
              "      <td>0.065056</td>\n",
              "      <td>0.926906</td>\n",
              "      <td>0.924877</td>\n",
              "      <td>0.928943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.036000</td>\n",
              "      <td>0.078396</td>\n",
              "      <td>0.935175</td>\n",
              "      <td>0.937833</td>\n",
              "      <td>0.932532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.015100</td>\n",
              "      <td>0.066233</td>\n",
              "      <td>0.945860</td>\n",
              "      <td>0.944211</td>\n",
              "      <td>0.947515</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [432/432 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.17521657049655914,\n",
              " 'eval_f1': 0.8811644779134907,\n",
              " 'eval_precision': 0.8723520320029093,\n",
              " 'eval_recall': 0.8901567863438167,\n",
              " 'eval_runtime': 9.3875,\n",
              " 'eval_samples_per_second': 367.83,\n",
              " 'eval_steps_per_second': 46.019,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcHoAEQKfKs4"
      },
      "source": [
        "8- Model Saving & Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_XgSKjfNEd",
        "outputId": "b2445956-cab0-45a9-f576-4eaba8a3d1ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Token Classification Output:\n",
            "{'entity': 'B-PER', 'score': 0.95738465, 'index': 1, 'word': 'El', 'start': 0, 'end': 2}\n",
            "{'entity': 'B-PER', 'score': 0.98268753, 'index': 2, 'word': '##on', 'start': 2, 'end': 4}\n",
            "{'entity': 'I-PER', 'score': 0.9949065, 'index': 3, 'word': 'Mu', 'start': 5, 'end': 7}\n",
            "{'entity': 'I-PER', 'score': 0.99306446, 'index': 4, 'word': '##sk', 'start': 7, 'end': 9}\n",
            "{'entity': 'B-ORG', 'score': 0.9387017, 'index': 9, 'word': 'Te', 'start': 24, 'end': 26}\n",
            "{'entity': 'B-ORG', 'score': 0.87641644, 'index': 10, 'word': '##sla', 'start': 26, 'end': 29}\n",
            "{'entity': 'B-LOC', 'score': 0.9974934, 'index': 16, 'word': 'Pa', 'start': 57, 'end': 59}\n",
            "{'entity': 'B-LOC', 'score': 0.9968714, 'index': 17, 'word': '##lo', 'start': 59, 'end': 61}\n",
            "{'entity': 'I-LOC', 'score': 0.9965964, 'index': 18, 'word': 'Alto', 'start': 62, 'end': 66}\n"
          ]
        }
      ],
      "source": [
        "model_path = \"./ner_trained_model\"\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)\n",
        "\n",
        "trained_model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "trained_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "ner_pipeline = pipeline(\"token-classification\", model=trained_model, tokenizer=trained_tokenizer)\n",
        "\n",
        "output = ner_pipeline(\"Elon Musk is the CEO of Tesla, which is headquartered in Palo Alto.\")\n",
        "print(\"\\nToken Classification Output:\")\n",
        "for entity in output:\n",
        "    print(entity)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
